{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"C7Ki_MN4NrIr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741798333910,"user_tz":-330,"elapsed":5941,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"b81fedf4-edca-4c96-ba98-85b483b69e6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping addict as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n","Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Installing collected packages: addict\n","Successfully installed addict-2.4.0\n"]}],"source":["!pip uninstall -y addict\n","!pip install addict\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QW_rDjSvNuGi","executionInfo":{"status":"ok","timestamp":1741798334707,"user_tz":-330,"elapsed":795,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"4f6832f8-16c3-4400-ecf1-3cdfa53fc1a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: addict\n","Version: 2.4.0\n","Summary: Addict is a dictionary whose items can be set using both attribute and item syntax.\n","Home-page: https://github.com/mewwts/addict\n","Author: Mats Julian Olsen\n","Author-email: mats@plysjbyen.net\n","License: UNKNOWN\n","Location: /usr/local/lib/python3.11/dist-packages\n","Requires: \n","Required-by: \n"]}],"source":["!pip show addict\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ROyDeNiNOaux","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741798337154,"user_tz":-330,"elapsed":119,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"3efb4093-c663-4f23-a440-213755878aea"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/usr/local/lib/python3.10/dist-packages/addict/addict.py': No such file or directory\n"]}],"source":["!cp /usr/local/lib/python3.10/dist-packages/addict/addict.py /content/addict.py\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1741798338813,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"},"user_tz":-330},"id":"XkCZYYamOfpH","outputId":"5c52958b-6e5a-4037-e058-dc2377994042"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/addict.py\n"]}],"source":["%%writefile /content/addict.py\n","import collections.abc\n","\n","class Dict(collections.abc.MutableMapping):\n","    def __init__(self, *args, **kwargs):\n","        self._data = dict(*args, **kwargs)\n","\n","    def __getitem__(self, key):\n","        return self._data[key]\n","\n","    def __setitem__(self, key, value):\n","        self._data[key] = value\n","\n","    def __delitem__(self, key):\n","        del self._data[key]\n","\n","    def __iter__(self):\n","        return iter(self._data)\n","\n","    def __len__(self):\n","        return len(self._data)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1741798340094,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"},"user_tz":-330},"id":"VOnrTihQOm9m","outputId":"8e9cb270-aaab-4091-e3d8-9658ce5aed31"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f76cfd43-2cf3-4ea2-9665-af638319e66b\", \"addict.py\", 461)"]},"metadata":{}}],"source":["from google.colab import files\n","files.download('/content/addict.py')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"HC7TdGsxO3xr","colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1741798350846,"user_tz":-330,"elapsed":7554,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"5f8e2bab-a031-483e-8222-881b885b4c43"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-ba35bce4-852c-4b46-8b96-3eab0955e55b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-ba35bce4-852c-4b46-8b96-3eab0955e55b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving addict (2).py to addict (2).py\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"JTMA0-QZPAUs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741798357772,"user_tz":-330,"elapsed":123,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"7b1d9a77-5289-4668-eb81-3dc29a8f3c40"},"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot create regular file '/usr/local/lib/python3.10/dist-packages/addict/addict.py': No such file or directory\n"]}],"source":["!cp /content/addict.py /usr/local/lib/python3.10/dist-packages/addict/addict.py\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WY4e73ZPDFy"},"outputs":[],"source":["import os\n","os.kill(os.getpid(), 9)\n"]},{"cell_type":"code","source":["!pip install torch torchvision albumentations numpy seaborn matplotlib tqdm addict pillow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbjaoVFHRphp","executionInfo":{"status":"ok","timestamp":1741798422841,"user_tz":-330,"elapsed":6222,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"1c89cf3a-3619-4f2c-a558-2264ca1ec1d2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cpu)\n","Collecting albumentations\n","  Downloading albumentations-2.0.5-py3-none-any.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: addict in /usr/local/lib/python3.11/dist-packages (2.4.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.14.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n","Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n","Collecting albucore==0.0.23 (from albumentations)\n","  Downloading albucore-0.0.23-py3-none-any.whl.metadata (5.3 kB)\n","Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n","  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Collecting stringzilla>=3.10.4 (from albucore==0.0.23->albumentations)\n","  Downloading stringzilla-3.12.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.23->albumentations)\n","  Downloading simsimd-6.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading albumentations-2.0.5-py3-none-any.whl (290 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.6/290.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading albucore-0.0.23-py3-none-any.whl (14 kB)\n","Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading simsimd-6.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (632 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.7/632.7 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading stringzilla-3.12.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (307 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.6/307.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: stringzilla, simsimd, opencv-python-headless, albucore, albumentations\n","Successfully installed albucore-0.0.23 albumentations-2.0.5 opencv-python-headless-4.11.0.86 simsimd-6.2.1 stringzilla-3.12.3\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tZ_3YMM_Ej_w","executionInfo":{"status":"ok","timestamp":1741798446680,"user_tz":-330,"elapsed":20630,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}}},"outputs":[],"source":["\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import models\n","from PIL import Image\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","import numpy as np\n","import os\n","import random\n","from collections.abc import MutableMapping\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import logging\n","from addict import Dict\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hsLU1OsCEr7_","executionInfo":{"status":"ok","timestamp":1741798450394,"user_tz":-330,"elapsed":58,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}}},"outputs":[],"source":["def seed_everything(seed:int=42) -> None:\n","    random.seed(seed)\n","    os.environ['PYTHONASSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","def get_optimizer(model:torch.nn.Module, name:str=\"SGD\", parameters:dict={}) -> torch.optim.Optimizer:\n","    optimizers = {\n","        \"SGD\": torch.optim.SGD,\n","        \"AdamW\": torch.optim.AdamW,\n","        \"Adam\": torch.optim.Adam,\n","        \"RMSprop\": torch.optim.RMSprop,\n","    }\n","    instance = optimizers.get(name, \"SGD\")\n","    optimizer = instance(model.parameters(), **parameters)\n","    return optimizer\n","\n","def get_scheduler(optimizer:torch.optim.Optimizer, name:str, parameters:dict):\n","    schedulers = {\n","        \"ReduceLROnPlateau\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n","        \"LambdaLR\": torch.optim.lr_scheduler.LambdaLR,\n","        \"StepLR\": torch.optim.lr_scheduler.StepLR,\n","        \"ExponentialLR\": torch.optim.lr_scheduler.ExponentialLR,\n","        \"MultiplicativeLR\": torch.optim.lr_scheduler.MultiplicativeLR,\n","        \"MultiStepLR\": torch.optim.lr_scheduler.MultiStepLR,\n","    }\n","    instance = schedulers[name]\n","    scheduler = instance(optimizer, **parameters)\n","    return scheduler\n","\n","def accuracy_score(predictions:torch.Tensor, targets:torch.Tensor) -> torch.Tensor:\n","    amount = (predictions == targets).sum()\n","    accuracy = amount / targets.size(0)\n","    return accuracy\n","\n","def hide_spines(ax, spines=[\"top\", \"right\", \"left\", \"bottom\"]):\n","    for spine in spines:\n","        ax.spines[spine].set_visible(False)\n","\n","def get_logger(name:str=__name__, format:str=\"[%(asctime)s][%(levelname)s]: %(message)s\") -> logging.Logger:\n","    logger = logging.getLogger(name)\n","    logger.setLevel(logging.INFO)\n","    formatter = logging.Formatter(format)\n","    file_handler = logging.FileHandler(name)\n","    file_handler.setLevel(logging.INFO)\n","    file_handler.setFormatter(formatter)\n","    stream_handler = logging.StreamHandler()\n","    stream_handler.setLevel(logging.INFO)\n","    stream_handler.setFormatter(formatter)\n","    logger.addHandler(stream_handler)\n","    logger.addHandler(file_handler)\n","    logger.propagate = False\n","    return logger\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Aj_DsmJfFMaT","executionInfo":{"status":"ok","timestamp":1741798452391,"user_tz":-330,"elapsed":5,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}}},"outputs":[],"source":["import os\n","import numpy as np\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","class PlantDiseaseDataset(Dataset):\n","    def __init__(self, path, augmentations=None, image_shape=(256, 256), channels=\"RGB\"):\n","        self.__images_labels = []\n","        self.image_shape = image_shape\n","        self.channels = channels\n","        self.augmentations = augmentations\n","\n","        if os.path.exists(path):\n","            self.labels = [label for label in os.listdir(path) if os.path.isdir(os.path.join(path, label))]\n","            print(f\"Labels found: {self.labels}\")\n","            for label in self.labels:\n","                label_path = os.path.join(path, label)\n","                files = [file for file in os.listdir(label_path) if file.lower().endswith(('jpg', 'png'))]\n","                if not files:\n","                    print(f\"No valid image files found in {label_path}\")\n","                for file in files:\n","                    image_path = os.path.join(label_path, file)\n","                    self.__images_labels.append((image_path, label))\n","            print(f\"Total images: {len(self.__images_labels)}\")\n","        else:\n","            raise ValueError(f\"Path {path} does not exist.\")\n","\n","    def __len__(self):\n","        return len(self.__images_labels)\n","\n","    def __getitem__(self, index):\n","        path, label = self.__images_labels[index]\n","        image = self._load(path)\n","\n","        if self.augmentations is not None:\n","            image = image.permute(1, 2, 0).numpy()\n","            image = self.augmentations(image=image)[\"image\"]\n","\n","        label = self.labels.index(label)\n","\n","        return {\"image\": image, \"label\": label}\n","\n","    def _load(self, path):\n","        width, height = self.image_shape\n","        loader = A.Compose([\n","            A.Resize(width=width, height=height),\n","            ToTensorV2(),\n","        ])\n","\n","        image_array = np.array(Image.open(path).convert(self.channels))\n","        return loader(image=image_array)[\"image\"]\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3cRhGG9vFS16","executionInfo":{"status":"ok","timestamp":1741798454598,"user_tz":-330,"elapsed":235,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class PlantDiseaseModel(nn.Module):\n","    def __init__(self, classes=2):\n","        super(PlantDiseaseModel, self).__init__()\n","        self.model = models.resnet34(pretrained=True)\n","\n","        for parameter in self.model.parameters():\n","            parameter.requires_grad = False\n","\n","        in_features = self.model.fc.in_features\n","        self.model.fc = nn.Sequential(\n","            nn.Linear(in_features=in_features, out_features=classes),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, image):\n","        output = self.model(image)\n","        return output\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"VNap10MQFf67","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741798477619,"user_tz":-330,"elapsed":21119,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"ae607a1e-31e0-4780-8a29-2438e910041b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"maanhxErFY51","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741798623520,"user_tz":-330,"elapsed":1523,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"9b756a7b-cd8c-4474-878a-5b1f43aeb25d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Labels found: ['Anthracnose lesions', 'Leaf MInner', 'Yellow Stuning', 'Healthy', 'Downy Mildew', 'Powdred']\n","Total images: 946\n","Labels found: ['Anthracnose lesions', 'Downy Mildew', 'Healthy', 'Leaf MInner', 'Powdred', 'Yellow Stuning']\n","Total images: 76\n","Labels found: ['Anthracnose lesions', 'Downy Mildew', 'Healthy', 'Leaf MInner', 'Powdred', 'Yellow Stuning']\n","Total images: 127\n"]}],"source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.utils.data import DataLoader\n","\n","\n","train_dataset = PlantDiseaseDataset(\n","    path=\"/content/drive/My Drive/cucumber_dataset /train/train\",\n","    image_shape=(128, 128),\n","    channels=\"RGB\",\n","    augmentations=A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        ToTensorV2()\n","    ])\n",")\n","\n","validation_dataset = PlantDiseaseDataset(\n","    path=\"/content/drive/My Drive/cucumber_dataset /Validation/Validation\",\n","    image_shape=(128, 128),\n","    channels=\"RGB\",\n","    augmentations=A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        ToTensorV2()\n","    ])\n",")\n","\n","test_dataset = PlantDiseaseDataset(\n","    path=\"/content/drive/My Drive/cucumber_dataset /test/test\",\n","    image_shape=(128, 128),\n","    channels=\"RGB\",\n","    augmentations=A.Compose([\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        ToTensorV2()\n","    ])\n",")\n","\n","train_loader = DataLoader(\n","    dataset=train_dataset,\n","    batch_size=32,\n","    num_workers=2,\n","    pin_memory=True,\n","    shuffle=True\n",")\n","\n","validation_loader = DataLoader(\n","    dataset=validation_dataset,\n","    batch_size=64,\n","    num_workers=2,\n","    pin_memory=True,\n","    shuffle=False\n",")\n","\n","test_loader = DataLoader(\n","    dataset=test_dataset,\n","    batch_size=64,\n","    num_workers=2,\n","    pin_memory=True,\n","    shuffle=False\n",")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Qm1PqZtWFVW5","executionInfo":{"status":"ok","timestamp":1741798626884,"user_tz":-330,"elapsed":131,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}}},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","from addict import Dict\n","\n","class Trainer:\n","    def __init__(self, model, criterion, optimizer, metric, scheduler=None, logger=None, device=\"cpu\"):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.logger = logger\n","        self.device = torch.device(device)\n","        self.metric = metric\n","        self.history = Dict({})\n","\n","    def __log(self, logs):\n","        for k, v in logs.items():\n","            if k not in self.history:\n","                self.history[k] = []\n","            self.history[k].append(v)\n","\n","    def evaluate(self, loader):\n","        loss, score, length = 0, 0, len(loader)\n","\n","        self.model.to(self.device)\n","        with torch.no_grad():\n","            loop = tqdm(loader, position=0, colour=\"BLACK\", desc=\"Evaluating\", leave=True)\n","            for batch in loop:\n","                if self.device.type != \"cpu\": torch.cuda.empty_cache()\n","                self.model.eval()\n","\n","                images = batch[\"image\"].float().to(self.device)\n","                labels = batch[\"label\"].long().to(\"cpu\")\n","\n","                probabilities = self.model(images).float().to(\"cpu\")\n","                predictions = torch.argmax(probabilities, dim=1).detach()\n","\n","                batch_loss = self.criterion(probabilities, labels)\n","                loss += batch_loss.item()\n","\n","                batch_score = self.metric(predictions, labels).item()\n","                score += batch_score\n","\n","            loss /= length\n","            score /= length\n","\n","        return loss, score\n","\n","    def fit(self, train_loader, validation_loader=None, epochs=10):\n","        self.model.to(self.device)\n","        train_length = len(train_loader)\n","\n","        for epoch in range(epochs):\n","            epoch_loss, epoch_score = 0, 0\n","\n","            loop = tqdm(train_loader, position=0, colour=\"BLACK\", leave=True, desc=f\"Epoch [{epoch+1}/{epochs}]: \")\n","            for batch in loop:\n","                if self.device.type != \"cpu\": torch.cuda.empty_cache()\n","                self.optimizer.zero_grad()\n","                self.model.train()\n","\n","                images = batch[\"image\"].float().to(self.device)\n","                labels = batch[\"label\"].long().to(\"cpu\")\n","\n","                probabilities = self.model(images).float().to(\"cpu\")\n","                predictions = torch.argmax(probabilities, dim=1).detach()\n","\n","                batch_loss = self.criterion(probabilities, labels)\n","                epoch_loss += batch_loss.item()\n","\n","                batch_score = self.metric(predictions, labels).item()\n","                epoch_score += batch_score\n","\n","                batch_loss.backward()\n","                self.optimizer.step()\n","\n","            epoch_loss /= train_length\n","            epoch_score /= train_length\n","\n","            self.__log({\"train_losses\": epoch_loss, \"train_scores\": epoch_score})\n","            if self.logger is not None:\n","                self.logger.info(f\"Epoch [{epoch+1}/{epochs}]: Loss: {epoch_loss} | Metric: {epoch_score}\")\n","\n","            if validation_loader is not None:\n","                validation_loss, validation_score = self.evaluate(validation_loader)\n","                self.__log({\"validation_losses\": validation_loss, \"validation_scores\": validation_score})\n","                if self.logger is not None:\n","                    self.logger.info(f\"Validation Epoch [{epoch+1}/{epochs}]: Loss: {validation_loss} | Metric: {validation_score}\")\n","\n","            if self.scheduler is not None:\n","                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","                    self.scheduler.step(validation_loss)\n","                else:\n","                    self.scheduler.step()\n","\n","                if self.logger is not None:\n","                    lr = self.optimizer.param_groups[0][\"lr\"]\n","                    self.logger.info(f\"Epoch [{epoch+1}/{epochs}] Learning Rate: {lr}\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"CUSdi9sIoQom","colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"status":"error","timestamp":1741798655121,"user_tz":-330,"elapsed":24394,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"3a16bef1-d1ad-40f8-9439-ab0293970b02"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n","100%|██████████| 83.3M/83.3M [00:00<00:00, 199MB/s]\n","Epoch [1/10]:   0%|\u001b[30m          \u001b[0m| 0/30 [00:20<?, ?it/s]\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'float' object has no attribute 'item'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-68f77e60f177>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-a31c5e4a6ee7>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, validation_loader, epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mbatch_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mepoch_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"]}],"source":["import torch.nn as nn\n","from sklearn.metrics import accuracy_score\n","train_config = {\n","    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","    \"epochs\": 10,\n","    \"seed\": 2021,\n","    \"image_shape\": (128, 128),\n","    \"image_channels\": 3,\n","    \"num_workers\": 2,\n","    \"batch_size\": 32,\n","    \"optimizer\": {\n","        \"type\": \"AdamW\",\n","        \"parameters\": {\n","            \"lr\": 0.001,\n","            \"weight_decay\": 0.01,\n","        }\n","    },\n","    \"scheduler\": {\n","        \"type\": \"ReduceLROnPlateau\",\n","        \"parameters\": {\n","            \"patience\": 2,\n","            \"mode\": \"min\",\n","            \"factor\": 0.1,\n","        }\n","    }\n","}\n","model = PlantDiseaseModel(classes=len(train_dataset.labels))\n","criterion = nn.CrossEntropyLoss()\n","optimizer = get_optimizer(\n","    model=model,\n","    name=train_config[\"optimizer\"][\"type\"],\n","    parameters=train_config[\"optimizer\"][\"parameters\"]\n",")\n","scheduler = None\n","if \"scheduler\" in train_config:\n","    scheduler = get_scheduler(\n","        optimizer=optimizer,\n","        name=train_config[\"scheduler\"][\"type\"],\n","        parameters=train_config[\"scheduler\"][\"parameters\"]\n","    )\n","trainer_logger = get_logger(\"trainer\")\n","trainer = Trainer(\n","    model=model,\n","    criterion=criterion,\n","    metric=accuracy_score,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    logger=trainer_logger,\n","    device=train_config[\"device\"]\n",")\n","trainer.fit(train_loader, validation_loader=validation_loader, epochs=train_config[\"epochs\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jXlteovrh693"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1sAQ50CFdYh"},"outputs":[],"source":["class PlantDiseaseModel(nn.Module):\n","    def __init__(self, classes=2):\n","        super(PlantDiseaseModel, self).__init__()\n","        self.model = models.resnet34(pretrained=True)\n","\n","        for parameter in self.model.parameters():\n","            parameter.require_grad = False\n","\n","        in_features = self.model.fc.in_features\n","        self.model.fc = nn.Sequential(\n","            nn.Linear(in_features=in_features, out_features=classes),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, image):\n","        output = self.model(image)\n","        return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Tf1OApQPR9Q"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","\n","class Trainer:\n","    def __init__(self, model, criterion, optimizer, metric, scheduler=None, logger=None, device=\"cpu\"):\n","        self.model = model\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.logger = logger\n","        self.device = torch.device(device)\n","        self.best_validation_loss = 0\n","        self.metric = metric\n","        self.history = Dict({})\n","\n","    def __log(self, logs):\n","        for k, v in logs.items():\n","            if k not in self.history:\n","                self.history[k] = []\n","            self.history[k].append(v)\n","\n","    def evaluate(self, loader):\n","        loss, score, length = 0, 0, len(loader)\n","\n","        self.model.to(self.device)\n","        with torch.no_grad():\n","            loop = tqdm(loader, position=0, colour=\"BLACK\", desc=f\"Evaluating: \", leave=True)\n","            for batch in loop:\n","                if self.device.type != \"cpu\": torch.cuda.empty_cache()\n","                self.model.eval()\n","                images = batch[\"image\"].float().to(self.device)\n","                labels = batch[\"label\"].long().to(\"cpu\")\n","\n","                probabilities = self.model(images).float().to(\"cpu\")\n","                predictions = torch.argmax(probabilities, dim=1).detach()\n","\n","                batch_loss = self.criterion(probabilities, labels)\n","                loss += batch_loss.item()\n","\n","                batch_score = self.metric(predictions, labels).item()\n","                score += batch_score\n","\n","            loss /= length\n","            score /= length\n","\n","        return loss, score\n","\n","    def fit(self, train_loader, validation_loader=None, epochs=10):\n","        self.model.to(self.device)\n","        train_length = len(train_loader)\n","\n","        for epoch in range(epochs):\n","            epoch_loss, epoch_score = 0, 0\n","\n","            loop = tqdm(train_loader, position=0, colour=\"BLACK\", leave=True, desc=f\"Epoch [{epoch+1}/{epochs}]: \")\n","            for batch in loop:\n","                if self.device.type != \"cpu\": torch.cuda.empty_cache()\n","                self.optimizer.zero_grad()\n","                self.model.train()\n","                images = batch[\"image\"].float().to(self.device)\n","                labels = batch[\"label\"].long().to(\"cpu\")\n","\n","                probabilities = self.model(images).float().to(\"cpu\")\n","                predictions = torch.argmax(probabilities, dim=1).detach()\n","\n","                batch_loss = self.criterion(probabilities, labels)\n","                epoch_loss += batch_loss.item()\n","\n","                batch_score = self.metric(predictions, labels).item()\n","                epoch_score += batch_score\n","\n","                batch_loss.backward()\n","                self.optimizer.step()\n","\n","            epoch_loss /= train_length\n","            epoch_score /= train_length\n","\n","            self.__log({\"train_losses\": epoch_loss, \"train_scores\": epoch_score})\n","            if self.logger is not None:\n","                self.logger.info(f\"Epoch [{epoch+1}/{epochs}]: Loss: {epoch_loss} | Metric: {epoch_score}\")\n","\n","            if validation_loader is not None:\n","                validation_loss, validation_score = self.evaluate(validation_loader)\n","                self.__log({\"validation_losses\": validation_loss, \"validation_scores\": validation_score})\n","                if self.logger is not None:\n","                    self.logger.info(f\"Validation Epoch [{epoch+1}/{epochs}]: Loss: {validation_loss} | Metric: {validation_score}\")\n","\n","                if self.scheduler is not None:\n","                    if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","                        self.scheduler.step(validation_loss)\n","                    else:\n","                        self.scheduler.step()\n","\n","                    if self.logger is not None:\n","                        lr = self.optimizer.param_groups[0][\"lr\"]\n","                        self.logger.info(f\"Epoch [{epoch+1}/{epochs}] Learning Rate: {lr}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf0AmQEPaIaY"},"outputs":[],"source":["  import torch\n","  import torch.nn as nn\n","  from sklearn.metrics import accuracy_score\n","  train_config = {\n","      \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n","      \"epochs\": 10,\n","      \"seed\": 2021,\n","      \"image_shape\": (128, 128),\n","      \"image_channels\": 3,\n","      \"num_workers\": 2,\n","      \"batch_size\": 32,\n","      \"optimizer\": {\n","          \"type\": \"AdamW\",\n","          \"parameters\": {\n","              \"lr\": 0.001,\n","              \"weight_decay\": 0.01,\n","          }\n","      },\n","      \"scheduler\": {\n","          \"type\": \"ReduceLROnPlateau\",\n","          \"parameters\": {\n","              \"patience\": 2,\n","              \"mode\": \"min\",\n","              \"factor\": 0.1,\n","          }\n","      }\n","  }\n","  model = PlantDiseaseModel(classes=len(train_dataset.labels))\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = get_optimizer(\n","      model=model,\n","      name=train_config[\"optimizer\"][\"type\"],\n","      parameters=train_config[\"optimizer\"][\"parameters\"]\n","  )\n","  scheduler = None\n","  if \"scheduler\" in train_config:\n","      scheduler = get_scheduler(\n","          optimizer=optimizer,\n","          name=train_config[\"scheduler\"][\"type\"],\n","          parameters=train_config[\"scheduler\"][\"parameters\"]\n","      )\n","  trainer_logger = get_logger(\"trainer\")\n","  trainer = Trainer(\n","      model=model,\n","      criterion=criterion,\n","      metric=accuracy_score,\n","      optimizer=optimizer,\n","      scheduler=scheduler,\n","      logger=trainer_logger,\n","      device=train_config[\"device\"]\n","  )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfJiZkscaNxV"},"outputs":[],"source":["seed_everything(train_config[\"seed\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kSnvo0LzcmG2"},"outputs":[],"source":["trainer.fit(train_loader, validation_loader=validation_loader, epochs=train_config[\"epochs\"])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzasI3gxlqgw"},"outputs":[],"source":["test_loss, test_accuracy = trainer.evaluate(test_loader)\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2Jwi8tSuczj"},"outputs":[],"source":["torch.save(model.state_dict(), \"/content/drive/My Drive/plant_disease_model.pth\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436,"status":"ok","timestamp":1724222108940,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"},"user_tz":-330},"id":"rNezUh0dulus","outputId":"9ce2be00-06da-4e5d-dce1-ed50ff0c122e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted Class: Anthracnose lesions\n"]}],"source":["import torch\n","from PIL import Image\n","import numpy as np\n","\n","model.eval()\n","sample_image = test_dataset[0][\"image\"].unsqueeze(0).to(train_config[\"device\"])\n","if sample_image.dtype != torch.float32:\n","    sample_image = sample_image.to(dtype=torch.float32)\n","\n","with torch.no_grad():\n","    prediction = model(sample_image)\n","    predicted_class = torch.argmax(prediction, dim=1)\n","    print(f\"Predicted Class: {test_dataset.labels[predicted_class.item()]}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":38},"id":"8cXEGqgcuu0S","outputId":"88dc2254-7643-406e-d92d-fea5c4429c09"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-7781c7b6-95e5-4fca-8d57-229025b09420\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-7781c7b6-95e5-4fca-8d57-229025b09420\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","from PIL import Image, ImageDraw, ImageFont\n","import torch\n","import numpy as np\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","import matplotlib.pyplot as plt\n","\n","def preprocess_image(image_path, image_shape=(128, 128), channels=\"RGB\"):\n","    loader = A.Compose([\n","        A.Resize(width=image_shape[0], height=image_shape[1]),\n","        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","        ToTensorV2(),\n","    ])\n","    image_array = np.array(Image.open(image_path).convert(channels))\n","    image_tensor = loader(image=image_array)[\"image\"]\n","    return image_tensor\n","\n","uploaded = files.upload()\n","image_path = next(iter(uploaded))\n","image_tensor = preprocess_image(image_path, image_shape=(128, 128), channels=\"RGB\").unsqueeze(0).to(train_config[\"device\"])\n","\n","model.eval()\n","with torch.no_grad():\n","    prediction = model(image_tensor)\n","    predicted_class = torch.argmax(prediction, dim=1).item()\n","    disease_probability = torch.softmax(prediction, dim=1)[0, predicted_class].item() * 100\n","\n","predicted_class_name = train_dataset.labels[predicted_class]\n","predicted_text = f\"{predicted_class_name} ({disease_probability:.2f}%)\"\n","\n","original_image = Image.open(image_path)\n","draw = ImageDraw.Draw(original_image)\n","font = ImageFont.load_default()\n","text_position = (10, 10)\n","draw.text(text_position, predicted_text, fill=\"red\", font=font)\n","\n","plt.figure(figsize=(16, 8))\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(Image.open(image_path))\n","plt.axis('off')\n","plt.title(\"Input Image\")\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(original_image)\n","plt.axis('off')\n","plt.title(f\"Predicted Class: {predicted_text}\")\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qqfcmq3r2Sbw"},"outputs":[],"source":["\n","from google.colab import files\n","from PIL import Image, ImageDraw, ImageFont\n","import torch\n","import numpy as np\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","import matplotlib.pyplot as plt\n","\n","\n","def preprocess_image(image_path, image_shape=(128, 128), channels=\"RGB\"):\n","    transform = A.Compose([\n","        A.Resize(height=image_shape[0], width=image_shape[1]),\n","        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n","        ToTensorV2(),\n","    ])\n","    image = Image.open(image_path).convert(channels)\n","    image_array = np.array(image)\n","    transformed = transform(image=image_array)\n","    image_tensor = transformed[\"image\"].unsqueeze(0)\n","    return image_tensor\n","\n","\n","uploaded = files.upload()\n","image_path = next(iter(uploaded.keys()))\n","\n","\n","image_tensor = preprocess_image(\n","    image_path,\n","    image_shape=(128, 128),\n","    channels=\"RGB\"\n",").to(train_config[\"device\"])\n","\n","\n","model.eval()\n","with torch.no_grad():\n","    output = model(image_tensor)\n","    probabilities = torch.softmax(output, dim=1)\n","    predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n","    predicted_probability = probabilities[0, predicted_class_idx].item() * 100\n","\n","\n","predicted_class_name = train_dataset.labels[predicted_class_idx]\n","predicted_text = f\"{predicted_class_name} ({predicted_probability:.2f}%)\"\n","\n","original_image = Image.open(image_path).convert(\"RGB\")\n","draw = ImageDraw.Draw(original_image)\n","font = ImageFont.truetype(\"arial.ttf\", size=24)\n","text_position = (10, 10)\n","text_color = (255, 0, 0)\n","draw.text(text_position, predicted_text, fill=text_color, font=font)\n","\n","\n","plt.figure(figsize=(16, 8))\n","\n","\n","plt.subplot(1, 2, 1)\n","plt.imshow(Image.open(image_path))\n","plt.axis('off')\n","plt.title(\"Input Image\")\n","\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(original_image)\n","plt.axis('off')\n","plt.title(f\"Predicted: {predicted_text}\")\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiBhY-DvvHwx"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","epochs = range(1, len(trainer.history[\"train_scores\"]) + 1)\n","train_scores = trainer.history[\"train_scores\"]\n","validation_scores = trainer.history[\"validation_scores\"]\n","plt.figure(figsize=(10, 5))\n","plt.plot(epochs, train_scores, 'b-', label='Training Accuracy')\n","plt.plot(epochs, validation_scores, 'r-', label='Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training and Validation Accuracy Over Epochs')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","source":["!pip install tensorflow numpy matplotlib\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xc_4MwTRTrxc","executionInfo":{"status":"ok","timestamp":1741799040880,"user_tz":-330,"elapsed":44463,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"f8df5237-26f4-47a1-d031-7bbcd74e0125"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow\n","  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Collecting astunparse>=1.6.0 (from tensorflow)\n","  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n","Collecting flatbuffers>=24.3.25 (from tensorflow)\n","  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Collecting google-pasta>=0.1.1 (from tensorflow)\n","  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n","Collecting libclang>=13.0.0 (from tensorflow)\n","  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n","Collecting tensorboard~=2.19.0 (from tensorflow)\n","  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n","Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n","  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n","  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n","Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n","  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n","Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n","  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n","Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n","  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m577.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n","Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n","Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, markdown, google-pasta, tensorboard, astunparse, tensorflow\n","Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 markdown-3.7 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define dataset paths\n","train_dir = \"/content/drive/My Drive/cucumber_dataset /train/train\"\n","val_dir = \"/content/drive/My Drive/cucumber_dataset /Validation/Validation\"\n","test_dir = \"/content/drive/My Drive/cucumber_dataset /test/test\"\n","\n","# Verify paths exist\n","assert os.path.exists(train_dir), \"Train directory not found!\"\n","assert os.path.exists(val_dir), \"Validation directory not found!\"\n","assert os.path.exists(test_dir), \"Test directory not found!\"\n","\n","# Image augmentation and normalization\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","val_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Load dataset\n","batch_size = 32\n","img_size = (128, 128)\n","train_generator = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n","val_generator = val_datagen.flow_from_directory(val_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n","test_generator = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n","\n","# Number of classes\n","num_classes = len(train_generator.class_indices)\n","print(f\"Number of classes: {num_classes}\")\n","\n","# Define CNN model\n","model = Sequential([\n","    Conv2D(32, (3,3), activation='relu', input_shape=(128, 128, 3)),\n","    MaxPooling2D(2,2),\n","    Conv2D(64, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    Conv2D(128, (3,3), activation='relu'),\n","    MaxPooling2D(2,2),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.4),\n","    Dense(num_classes, activation='softmax')\n","])\n","\n","# Compile model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","\n","# Train model\n","epochs = 10\n","history = model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n","\n","# Save model\n","model_path = \"/content/drive/My Drive/cucumber_model.h5\"\n","model.save(model_path)\n","print(f\"Model saved as {model_path}\")\n","\n","# Load and evaluate model\n","loaded_model = load_model(model_path)\n","test_loss, test_acc = loaded_model.evaluate(test_generator)\n","print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"-vOkSqB3T3e1","executionInfo":{"status":"ok","timestamp":1741799612571,"user_tz":-330,"elapsed":511356,"user":{"displayName":"THOTA KISHAN BU21CSEN0500099","userId":"12151316567603426906"}},"outputId":"27b80b0f-a909-44f2-ecbc-c03a664d0d9d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Found 946 images belonging to 6 classes.\n","Found 76 images belonging to 6 classes.\n","Found 127 images belonging to 6 classes.\n","Number of classes: 6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m3,211,392\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │             \u001b[38;5;34m774\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,305,414\u001b[0m (12.61 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,414</span> (12.61 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,305,414\u001b[0m (12.61 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,414</span> (12.61 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 6s/step - accuracy: 0.1709 - loss: 1.9018 - val_accuracy: 0.1974 - val_loss: 1.7317\n","Epoch 2/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.2979 - loss: 1.6242 - val_accuracy: 0.2500 - val_loss: 1.5018\n","Epoch 3/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 997ms/step - accuracy: 0.3722 - loss: 1.4640 - val_accuracy: 0.2632 - val_loss: 1.6394\n","Epoch 4/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 996ms/step - accuracy: 0.4066 - loss: 1.3619 - val_accuracy: 0.3289 - val_loss: 1.3797\n","Epoch 5/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 992ms/step - accuracy: 0.4606 - loss: 1.2613 - val_accuracy: 0.2632 - val_loss: 1.5576\n","Epoch 6/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - accuracy: 0.4577 - loss: 1.2950 - val_accuracy: 0.2632 - val_loss: 1.5307\n","Epoch 7/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1000ms/step - accuracy: 0.4873 - loss: 1.2743 - val_accuracy: 0.3684 - val_loss: 1.4161\n","Epoch 8/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.4689 - loss: 1.2720 - val_accuracy: 0.2895 - val_loss: 1.5651\n","Epoch 9/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - accuracy: 0.5111 - loss: 1.2031 - val_accuracy: 0.3158 - val_loss: 1.3518\n","Epoch 10/10\n","\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 999ms/step - accuracy: 0.4823 - loss: 1.2255 - val_accuracy: 0.3421 - val_loss: 1.3806\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["Model saved as /content/drive/My Drive/cucumber_model.h5\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4s/step - accuracy: 0.4034 - loss: 1.5248\n","Test Accuracy: 43.31%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qzPRZ9tCUDa_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1b-Pk1cUchIehdi4SvXYjMB42SETDI2v_","timestamp":1723963866386}],"gpuType":"V28","mount_file_id":"1TgA3eL4oEaNw_Q75IRg68eEFR6pL3eDd","authorship_tag":"ABX9TyMwLnaVyBrj85MFZk0axzbZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}